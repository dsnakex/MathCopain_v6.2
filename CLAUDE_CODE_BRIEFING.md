# ü§ñ CLAUDE_CODE_BRIEFING.md
## Guide d'Utilisation Claude Code pour Phases 6-8

---

# üéØ OBJECTIF

Fournir √† Claude Code des **prompts optimis√©s et contextualis√©s** pour impl√©menter les phases 6, 7, 8 de MathCopain sans friction.

**Chaque prompt inclut:**
- ‚úÖ Code structure attendue
- ‚úÖ Fichiers √† cr√©er
- ‚úÖ Tests √† √©crire
- ‚úÖ D√©pendances
- ‚úÖ Commits git

---

# üìã PHASE 6 : Fondations P√©dagogiques

## 6.1 - FEEDBACK P√âDAGOGIQUE INTELLIGENT

### Prompt 6.1.1 - ErrorAnalyzer

**Titre:** "Impl√©menter ErrorAnalyzer pour MathCopain v6.4"

**Texte du Prompt:**

```
# CONTEXT
Je d√©veloppe MathCopain, une app d'apprentissage des maths pour enfants CE1-CM2.
Vous avez d√©j√† fait une v6.3 compl√®te (tests, s√©curit√©, architecture modulaire).
Maintenant on lance la Phase 6 : Fondations P√©dagogiques.

# PHASE 6.1 - FEEDBACK P√âDAGOGIQUE INTELLIGENT

## Objectif
Transformer feedback basique ‚Üí explications transformatives qui augmentent l'apprentissage de +35-40%
Bas√© sur th√©orie Hattie 2008 (feedback avec effet-taille 0.79).

## T√ÇCHE: Impl√©menter ErrorAnalyzer

### Fichier √† cr√©er
`core/pedagogy/error_analyzer.py` (300 lignes)

### Structure Classe

```python
class ErrorAnalyzer:
    """Analyse errors math√©matiques et cat√©gorise par type"""
    
    def __init__(self):
        # Load error_taxonomy.json
        # 500+ erreurs pr√©-catalogu√©es
        self.error_catalog = self._load_catalog()
    
    def analyze_error_type(self, exercise, response, expected):
        '''
        Retourne:
        {
            "type": "Conceptual" | "Procedural" | "Calculation",
            "misconception": "L'enfant oublie la retenue",
            "severity": 1-5,
            "confidence": 0.92,
            "examples": [...]
        }
        '''
    
    def identify_misconception(self, error_type):
        '''
        Query error_taxonomy.json
        Retourne: misconception details + common student reasons
        '''
    
    def root_cause_analysis(self, error_details):
        '''
        Pourquoi l'enfant s'est tromp√©?
        Retourne: analysis + prerequisite gaps
        '''
```

### D√©pendances
- json, pandas, numpy

### Data Files
- `data/error_taxonomy.json` (will be created separately)

### Tests √† √©crire
`tests/test_error_analyzer.py` (300+ tests)

Coverage: 85%+
```

**Note pour Claude Code:**
Ceci est le premier prompt. Il doit cr√©er ErrorAnalyzer complet avec tests.
Attendez la confirmation qu'il compile + tous les tests passent avant de continuer.

---

### Prompt 6.1.2 - error_taxonomy.json Creation

**Titre:** "G√©n√©rer error_taxonomy.json avec 500+ erreurs math√©matiques"

**Texte:**

```
# T√ÇCHE: Cr√©er error_taxonomy.json

## Structure JSON

Le fichier doit contenir 500+ erreurs math√©matiques structur√©es par domaine.

### Format

```json
{
  "error_catalog": {
    "addition_carry_error_001": {
      "type": "procedural",
      "domain": "addition",
      "common_in_grades": ["CE1", "CE2"],
      "misconception": "L'enfant oublie la retenue quand la somme d√©passe 10",
      "examples": [
        {
          "input": "23 + 14",
          "wrong_answer": "37",
          "correct_answer": "37",
          "error_description": "Oublie de reporter la dizaine"
        }
      ],
      "feedback_templates": [
        "Tu as trouv√© 37, mais regarde: 23 + 14 = (20 + 10) + (3 + 4) = 30 + 7 = 37 ‚úì",
        "N'oublie pas la retenue! Quand tu additionnes 3 + 4 = 7, √ßa va dans les unit√©s."
      ],
      "remediation_path": "addition_with_carry_basics",
      "severity": 3
    },
    // ... 499+ more errors
  }
}
```

## Domaines √† couvrir (500+ total errors)

- addition (50+ errors): carry, place value, zero, negative
- subtraction (50+ errors): borrowing, place value, minuend vs subtrahend
- multiplication (50+ errors): tables, distributive, area model
- division (40+ errors): remainders, long division, divisibility
- fractions (40+ errors): equivalence, operations, GCD
- decimals (40+ errors): place value, operations, rounding
- geometry (40+ errors): perimeter, area, volume, angle
- measurements (40+ errors): units, conversion, estimation
- proportionality (40+ errors): ratio, percentage, scale
- money (40+ errors): currency, making change, comparison

## Production Requirements

1. Valid JSON (verify with Python json.loads())
2. Each error has: type, domain, misconception, examples, templates, remediation
3. Taxonomic structure (400 structured errors + variations)
4. No duplicates
5. Pedagogically accurate

## Output Format
Generate in single JSON block, ready to paste into data/error_taxonomy.json
```

**Note:** Ceci est long (~20,000 lignes JSON). Claude Code doit le g√©n√©rer en un seul fichier.

---

### Prompt 6.1.3 - FeedbackGenerator Implementation

**Titre:** "Impl√©menter FeedbackGenerator - Feedback P√©dagogique Transformatif"

**Texte:**

```
# T√ÇCHE: Impl√©menter FeedbackGenerator

Bas√© sur ErrorAnalyzer, g√©n√©rer feedback p√©dagogiquement transformatif.
Th√©orie: Hattie 2008 - Feedback avec effet-taille 0.79.

## Fichier √† cr√©er
`core/pedagogy/feedback_engine.py` (400 lignes)

## Structure Classe

```python
class TransformativeFeedback:
    """Generate pedagogically transformative feedback"""
    
    def __init__(self):
        self.error_analyzer = ErrorAnalyzer()
        self.feedback_gen = FeedbackGenerator()
        self.remediation = RemediationRecommender()
    
    def process_exercise_response(self, exercise, response, user_id):
        '''
        Analyse r√©ponse exercice ‚Üí Feedback multi-couches
        
        Retourne:
        {
            "immediate": "‚úÖ Correct!" ou "‚ùå Pas tout √† fait",
            "explanation": "Concept-level explanation",
            "strategy": "Strat√©gie alternative",
            "remediation": {...},
            "encouragement": "Personnalis√©",
            "next_action": "Refaire | Continuer | Voir d√©tails"
        }
        '''
    
    def _generate_success_feedback(self, exercise, user_id):
        '''Positive feedback intelligent'''
    
    def _generate_failure_feedback(self, error_analysis, user_id):
        '''Constructive failure feedback'''
```

## Multi-Layer Feedback

When response is WRONG:
1. **Immediate** (5 words): "C'est presque √ßa!"
2. **Explanation** (50 words): Pourquoi la r√©ponse est fausse
3. **Strategy** (50 words): Une autre fa√ßon de r√©soudre
4. **Remediation** (Action): Exercice similaire plus facile
5. **Encouragement** (Personalized): Bas√© sur l'historique

When response is CORRECT:
1. **Immediate**: "‚úÖ Exact!"
2. **Recognition**: Acknowledgement sp√©cifique
3. **Insight**: "Tu l'as r√©solu en 12 secondes!"
4. **Progression**: Visualiser progr√®s
5. **Next Challenge**: Proposer niveau suivant

## D√©pendances
- Jinja2 (pour templates)
- Pandas (pour donn√©es utilisateur)
- ErrorAnalyzer (class import√©e)

## Tests
`tests/test_feedback_engine.py` (400+ tests)

Coverage: 85%+

Test scenarios:
- Correct answer ‚Üí correct feedback
- Wrong answer ‚Üí diagnostic feedback
- Edge cases: Empty response, malformed input, unusual error types
```

---

### Prompt 6.1.4 - App.py Integration

**Titre:** "Int√©grer TransformativeFeedback dans app.py"

**Texte:**

```
# T√ÇCHE: Int√©grer TransformativeFeedback dans app.py existant

## Contexte
MathCopain a un app.py Streamlit de 305 lignes (refactoris√© en v6.3).
Vous devez ajouter feedback intelligent APR√àS exercice submission.

## Modifications app.py

Ajouter apr√®s ligne qui v√©rifie r√©ponse exercice:

```python
from core.pedagogy.feedback_engine import TransformativeFeedback

# Instancier une fois
if 'feedback_engine' not in st.session_state:
    st.session_state.feedback_engine = TransformativeFeedback()

# DANS exercise_completed_handler():
def show_exercise_feedback(exercise, response, user_id):
    feedback = st.session_state.feedback_engine.process_exercise_response(
        exercise=exercise,
        response=response,
        user_id=user_id
    )
    
    # Show feedback UI
    with st.container(border=True):
        # Layer 1: Immediate
        if feedback['success']:
            st.success(feedback['immediate'])
        else:
            st.error(feedback['immediate'])
        
        # Layer 2: Explanation (expander)
        with st.expander("üìñ Explication"):
            st.write(feedback['explanation'])
        
        # Layer 3: Strategy (expander)
        with st.expander("üí° Strat√©gie Alternative"):
            st.write(feedback['strategy'])
        
        # Layer 4: Remediation
        if feedback['remediation']:
            st.info(f"üìö {feedback['remediation']['title']}")
            st.write(feedback['remediation']['description'])
        
        # Layer 5: Encouragement
        st.caption(f"‚ú® {feedback['encouragement']}")
        
        # Next Action
        col1, col2 = st.columns([1, 1])
        with col1:
            if st.button("üîÑ Refaire ce type", key="replay"):
                # Regenerate same type
                pass
        with col2:
            if st.button("‚è≠Ô∏è Continuer", key="continue"):
                # Move to next exercise
                pass
```

## UI Structure
- 5 layers of feedback
- Expanders to avoid overwhelming
- Action buttons (Refaire, Continuer)
- Emoji for visual cues

## Tests
`tests/test_feedback_integration.py` (150+ tests)

Verify:
- Feedback renders correctly
- All layers display
- Buttons functional
- No console errors
```

---

## 6.2 - M√âTACOGNITION & AUTOR√âGULATION

### Prompt 6.2.1 - MetacognitionEngine

**Titre:** "Impl√©menter MetacognitionEngine - Questions r√©flexives post-exercice"

**Texte:**

```
# T√ÇCHE: MetacognitionEngine

Aider enfants √† r√©fl√©chir √† leur processus d'apprentissage.
Th√©orie: Flavell (m√©tacognition) - pr√©dicteur majeur succ√®s acad√©mique.

## Fichier √† cr√©er
`core/pedagogy/metacognition.py` (400 lignes)

## Classe MetacognitionEngine

```python
class MetacognitionEngine:
    def __init__(self, user_id):
        self.user_id = user_id
        self.portfolio = StrategyPortfolio(user_id)
    
    def generate_reflection_questions(self, exercise):
        '''
        Generate 4 reflection questions (30 sec max)
        
        1. Strat√©gie? ["Doigts", "Mental", "Dessin", "Formule", "Autre"]
        2. Difficult√©? [Facile ‚Üê slider ‚Üí Difficile]
        3. Auto-explication? ["Explique comment tu as trouv√©"]
        4. Intention future? ["Prochaine fois je vais..."]
        '''
    
    def process_reflection(self, reflection_data):
        '''
        Traite r√©ponses r√©flexives:
        1. Enregistrer strat√©gie dans portfolio
        2. Analyser patterns
        3. G√©n√©rer insights personnalis√©s
        4. Sugg√©rer am√©liorations
        '''
    
    def generate_self_regulation_suggestions(self):
        '''
        Suggest actions based on session:
        - "Tu sembles frustr√©. Pause?"
        - "5 bonnes d'affil√©e! D√©fi plus difficile?"
        - "Tu fatigues. Peut-√™tre c'est bon pour aujourd'hui."
        '''
```

## Reflection UI (30 sec max)

Question 1: "üéØ Quelle strat√©gie as-tu utilis√©e?"
  ‚òê Sur mes doigts
  ‚òê Dans ma t√™te (mental)
  ‚òê En dessinant
  ‚òê Avec une formule
  ‚òê Autrement: _____

Question 2: "üìä Difficult√©?"
  [Facile] ‚óê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚óë [Difficile]

Question 3: "üí¨ Comment tu as trouv√©?"
  [Textbox - optional]

Question 4: "üîÆ Prochaine fois?"
  [Textbox - optional]

## Data Storage
- Store in user_profiles/{user_id}/reflections.json
- Accumulate 50+ reflections for pattern analysis

## Tests
`tests/test_metacognition.py` (350+ tests)

Scenarios:
- Reflection capture
- Pattern detection
- Suggestion generation
- Portfolio updates
```

---

## 6.3 - PROFILING STYLES D'APPRENTISSAGE

### Prompt 6.3.1 - LearningStyleAnalyzer

**Titre:** "Impl√©menter LearningStyleAnalyzer - 5 Styles"

**Texte:**

```
# T√ÇCHE: LearningStyleAnalyzer

Identifier learning style de chaque enfant + adapter pr√©sentation.
Th√©orie: Gardner (Multiple Intelligences).

## Fichier √† cr√©er
`core/pedagogy/learning_style.py` (350 lignes)

## 5 Learning Styles

1. **Visual** - Pr√©f√®re graphiques, diagrammes, couleurs
2. **Auditory** - Pr√©f√®re descriptions verbales, audio
3. **Kinesthetic** - Pr√©f√®re manipuler, interactif, tactile
4. **Logical** - Pr√©f√®re comprendre pourquoi, causal chains
5. **Narrative** - Pr√©f√®re histoires, contextes r√©els

## Classe LearningStyleAnalyzer

```python
class LearningStyleAnalyzer:
    STYLES = ["visual", "auditory", "kinesthetic", "logical", "narrative"]
    
    def assess_from_quiz(self, responses):
        '''Quiz 5-7 questions ‚Üí Primary + Secondary style'''
    
    def infer_from_performance(self, performance_history):
        '''Analyser patterns historiques ‚Üí Infer style'''
    
    def combine_assessments(self, quiz_result, performance_result):
        '''
        Combine:
        - Quiz: 40%
        - Performance: 60%
        
        Retourne: {primary, secondary, confidence}
        '''
```

## Quiz Format (5-7 questions)

Question 1: "Quand tu apprends, tu pr√©f√®res:"
  ‚òê Voir un diagramme
  ‚òê √âcouter une explication
  ‚òê Essayer soi-m√™me
  ‚òê Comprendre la logique

Question 2: "Tes meilleurs souvenirs √©cole?"
  ‚òê Les tableaux/posters
  ‚òê Les histoires du prof
  ‚òê Les exp√©riences/activit√©s
  ‚òê Les maths/structures

... (3-5 more)

## Profile Storage
`data/user_profiles/{user_id}/learning_style.json`

```json
{
  "primary": {
    "style": "visual",
    "confidence": 0.87
  },
  "secondary": {
    "style": "kinesthetic",
    "confidence": 0.62
  },
  "assessment_date": "2025-11-15",
  "data_points": 45,
  "confidence_overall": 0.82
}
```

## Tests
`tests/test_learning_style.py` (300+ tests)

Coverage: 85%+
```

---

### Prompt 6.3.2 - ExerciseAdapters (5 Adapters)

**Titre:** "Impl√©menter 5 ExerciseAdapters pour adapter pr√©sentation par style"

**Texte:**

```
# T√ÇCHE: Impl√©menter 5 ExerciseAdapters

Adapter pr√©sentation d'exercice selon learning style.
+25-35% engagement quand adapt√©.

## Fichiers √† cr√©er
`core/exercise_generator/exercise_adapter.py` (Main adapter)
`core/exercise_generator/adapters/visual_adapter.py` (150 lignes)
`core/exercise_generator/adapters/auditory_adapter.py` (100 lignes)
`core/exercise_generator/adapters/kinesthetic_adapter.py` (150 lignes)
`core/exercise_generator/adapters/logical_adapter.py` (100 lignes)
`core/exercise_generator/adapters/narrative_adapter.py` (150 lignes)

## Main ExerciseAdapter

```python
class ExercisePresenterAdapter:
    def __init__(self, learning_style):
        self.style = learning_style
        self.adapters = {
            "visual": VisualAdapter(),
            "auditory": AuditoryAdapter(),
            "kinesthetic": KinestheticAdapter(),
            "logical": LogicalAdapter(),
            "narrative": NarrativeAdapter()
        }
    
    def adapt_exercise(self, exercise):
        '''
        Retourne exercise adapt√© au style:
        {
            "problem_statement": adapted string,
            "hint": adapted hint,
            "visual_aids": {...},
            "explanation_style": "...",
            "resource_suggestion": {...}
        }
        '''
```

## 5 Adapters Details

### VisualAdapter
```python
class VisualAdapter:
    def format_problem(self, problem):
        # Add emoji, formatting
        return "üìä Visualise: " + problem
    
    def generate_visuals(self, exercise):
        # Return: diagram, number_line, color_coding
    
    def suggest_resources(self):
        # Return URLs to visual explanation videos
```

### AuditoryAdapter
```python
class AuditoryAdapter:
    def format_problem(self, problem):
        return "üéµ √âcoute: " + problem
    
    def suggest_resources(self):
        # Return audio explanation file
```

### KinestheticAdapter
```python
class KinestheticAdapter:
    def format_problem(self, problem):
        return "üëÜ Manipule: " + problem
    
    def generate_visuals(self, exercise):
        # Return interactive manipulables (draggable blocks)
```

### LogicalAdapter
```python
class LogicalAdapter:
    def format_problem(self, problem):
        return "üß† Comprends la logique: " + problem
    
    def format_hint(self, hint):
        return "Pourquoi? " + hint
```

### NarrativeAdapter
```python
class NarrativeAdapter:
    def format_problem(self, problem):
        story = self._create_story_context(problem)
        return f"üìñ {story}: {problem}"
```

## Tests
`tests/test_exercise_adapter.py` (250+ tests)

Test each adapter with multiple exercise types.
Coverage: 85%+
```

---

# üìä PHASE 7 : Infrastructure & IA

## 7.1 - POSTGRESQL MIGRATION

### Prompt 7.1.1 - PostgreSQL Schema & SQLAlchemy Models

**Titre:** "Cr√©er PostgreSQL schema + SQLAlchemy models pour MathCopain"

**Texte:**

```
# T√ÇCHE: PostgreSQL Migration

Migrer de JSON ‚Üí PostgreSQL relational database.
Scalable pour 1000+ concurrent users.

## Fichiers √† cr√©er

`database/models.py` (350 lignes) - SQLAlchemy models
`database/connection.py` (150 lignes) - Connection pooling
`database/migrations/env.py` - Alembic environment
`database/migrations/versions/001_initial_schema.py` - Initial migration

## SQL Schema (PostgreSQL)

```sql
-- Users table
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    pin_hash VARCHAR(255) NOT NULL,
    learning_style VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW(),
    last_login TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE
);

-- Exercise Responses
CREATE TABLE exercise_responses (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    exercise_id VARCHAR(100),
    skill_domain VARCHAR(50),
    difficulty_level INTEGER,
    response TEXT,
    is_correct BOOLEAN,
    time_taken_seconds INTEGER,
    strategy_used VARCHAR(100),
    error_type VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_user_skill (user_id, skill_domain),
    INDEX idx_created (created_at)
);

-- Skill Profiles
CREATE TABLE skill_profiles (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) UNIQUE,
    skill_domain VARCHAR(50),
    proficiency_level FLOAT,
    exercises_completed INTEGER,
    success_rate FLOAT,
    last_practiced TIMESTAMP,
    
    INDEX idx_user (user_id)
);

-- Parent Accounts
CREATE TABLE parent_accounts (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    pin_hash VARCHAR(255),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Parent-Child Links
CREATE TABLE parent_child_links (
    id SERIAL PRIMARY KEY,
    parent_id INTEGER REFERENCES parent_accounts(id),
    child_id INTEGER REFERENCES users(id),
    permission_level VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
);

-- Analytics Events
CREATE TABLE analytics_events (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id),
    event_type VARCHAR(50),
    event_data JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_user_event (user_id, event_type),
    INDEX idx_created (created_at)
);
```

## SQLAlchemy Models

```python
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, ForeignKey
from sqlalchemy.orm import relationship

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    username = Column(String(50), unique=True, nullable=False)
    pin_hash = Column(String(255), nullable=False)
    learning_style = Column(String(20))
    created_at = Column(DateTime, server_default=func.now())
    
    exercise_responses = relationship("ExerciseResponse")

class ExerciseResponse(Base):
    __tablename__ = 'exercise_responses'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'))
    exercise_id = Column(String(100))
    skill_domain = Column(String(50))
    is_correct = Column(Boolean)
    time_taken_seconds = Column(Integer)
    created_at = Column(DateTime, server_default=func.now())

# ... more models
```

## Tests
`tests/test_db_models.py` (300+ tests)

- Model creation
- Relationship validation
- Constraint checking
```

---

### Prompt 7.1.2 - Data Migration Script

**Titre:** "Cr√©er json_to_postgres.py - Migration script s√©curis√©"

**Texte:**

```
# T√ÇCHE: Data Migration Script

Migrer donn√©es from JSON ‚Üí PostgreSQL, avec:
- Backup avant migration
- Dry-run mode
- Validation post-migration
- Rollback recovery

## Fichier √† cr√©er
`database/migration_scripts/json_to_postgres.py` (250 lignes)

## Features

```python
def migrate_data(source_dir, mode='dry-run'):
    '''
    Modes:
    - 'dry-run': Validate but don't commit
    - 'full': Actually migrate
    '''
    
    # 1. Backup JSON first
    backup_json_files(source_dir)
    
    # 2. Load JSON data
    users = load_json(f"{source_dir}/users_data.json")
    exercises = load_json(f"{source_dir}/exercises_history.json")
    skills = load_json(f"{source_dir}/skill_profiles.json")
    
    # 3. Validate data integrity
    validation_report = validate_data(users, exercises, skills)
    
    if not validation_report['valid']:
        print("‚ùå Validation failed. Not migrating.")
        return False
    
    # 4. Transform & Insert
    if mode == 'dry-run':
        print("DRY-RUN MODE")
        print(f"Would insert {len(users)} users")
        print(f"Would insert {len(exercises)} exercises")
        return True
    
    elif mode == 'full':
        session = get_session()
        try:
            for user in users:
                db_user = User(...)
                session.add(db_user)
            session.commit()
            print(f"‚úÖ Migrated {len(users)} users successfully")
        except Exception as e:
            session.rollback()
            print(f"‚ùå Migration failed: {e}")
            restore_from_backup()
```

## Rollback Strategy

If migration fails:
1. JSON backup exists (created before)
2. Restore from backup script
3. Verify recovery

## Tests
`tests/test_migration.py` (300+ tests)

- Dry-run validation
- Data integrity checks
- Rollback recovery
- Performance (>1000 rows)
```

---

## 7.2 - IA ADAPTIVE LEARNING

### Prompt 7.2.1 - DifficultyOptimizer

**Titre:** "Impl√©menter DifficultyOptimizer - ML pour difficult√© optimale"

**Texte:**

```
# T√ÇCHE: DifficultyOptimizer

Machine Learning model pour:
- Pr√©dire difficult√© optimale (D1-D5)
- Maintenir Flow state (70% success rate)
- Expliquer choix (XAI)

## Fichier √† cr√©er
`core/ml/difficulty_optimizer.py` (400 lignes)

## Model Type
Gradient Boosting (XGBoost ou LightGBM)

## Features (Input)

```python
features = [
    recent_success_rate,  # Last 10 exercises
    avg_time_taken,
    trend,  # +1 improving, -1 declining
    streak,  # Consecutive successes
    fatigue_level,  # 0-1
    learning_velocity,
    confidence_score,
    hour_of_day_performance,
    day_of_week_performance,
    prerequisite_mastery
]
```

## Output
- Predicted difficulty (1-5 continuous)
- Discretize to D1-D5
- Apply Flow Theory adjustment

## Flow Theory Integration

```python
def apply_flow_adjustment(difficulty, current_success_rate):
    target = 0.70  # 70% optimal for learning
    
    if current_success_rate > target + 0.15:
        difficulty += 1  # Trop facile
    elif current_success_rate < target - 0.15:
        difficulty -= 1  # Trop difficile
    
    return clip(difficulty, 1, 5)
```

## Explainability (XAI)

```python
def explain_difficulty_choice(user_id, difficulty):
    '''
    Retourne: explication humaine de pourquoi D3 par exemple
    "J'ai choisi difficult√© 3 car:
    ‚úì Tu r√©ussis bien (+75%)
    üìà Tu t'am√©liores
    üò¥ Tu fatigues un peu"
    '''
```

## Tests
`tests/test_ml_predictions.py` (300+ tests)

- Model accuracy
- Flow state maintenance
- Edge cases (extreme fatigue, etc.)

## Model Serving
- Train on historical data
- Save as .pkl
- Load in app.py for real-time predictions
```

---

### Prompt 7.2.2 - PerformancePredictor

**Titre:** "Impl√©menter PerformancePredictor - Ensemble ML"

**Texte:**

```
# T√ÇCHE: PerformancePredictor

Pr√©dire:
1. Probabilit√© de succ√®s
2. Identifier √©l√®ves √† risque (early intervention)
3. Timeline pour ma√Ætriser le domaine

## Fichier √† cr√©er
`core/ml/performance_predictor.py` (350 lignes)

## Models
- LSTM (40%) - Time series forecasting
- Random Forest (60%) - At-risk classification

## Ensemble Voting

```python
def predict_success_probability(features):
    lstm_pred = lstm_model.predict(features)  # 0-1
    rf_pred = rf_classifier.predict_proba(features)[1]  # 0-1
    
    ensemble = 0.4 * lstm_pred + 0.6 * rf_pred
    return clip(ensemble, 0, 1)
```

## At-Risk Learner Detection

```python
def identify_at_risk_learners(user_id, horizon_days=7):
    risk_score = calculate_risk_score(user_id, horizon_days)
    return risk_score > 0.6  # 60% threshold
```

## Mastery Timeline Prediction

```python
def predict_mastery_timeline(user_id, domain):
    current_proficiency = get_proficiency(user_id, domain)
    learning_velocity = get_velocity(user_id, domain)
    
    exercises_needed = (1.0 - current_proficiency) / learning_velocity
    days_to_mastery = exercises_needed / 2  # ~2 exercises/day
    
    return {
        "exercises_needed": int(exercises_needed),
        "estimated_days": int(days_to_mastery),
        "confidence": 0.82
    }
```

## Tests
`tests/test_performance_predictor.py` (300+ tests)

- Success probability calibration
- At-risk detection accuracy
- Timeline estimation accuracy
- Fairness across demographics
```

---

# üéì PHASE 8 : D√©ploiement Institutionnel

## 8.1 - MODE ENSEIGNANT & CLASSE

### Prompt 8.1.1 - ClassroomManager Backend

**Titre:** "Impl√©menter ClassroomManager - Gestion classes pour enseignants"

**Texte:**

```
# T√ÇCHE: ClassroomManager

Backend pour:
- Cr√©er classes
- Ajouter √©l√®ves
- Cr√©er assignments
- Monitorer progression temps r√©el

## Fichier √† cr√©er
`core/pedagogy/classroom_manager.py` (500 lignes)

## Classe ClassroomManager

```python
class ClassroomManager:
    def __init__(self, teacher_id):
        self.teacher_id = teacher_id
        self.db = DatabaseConnection()
    
    def create_classroom(self, name, class_level, max_students=30):
        '''Cr√©er nouvelle classe'''
    
    def add_student_to_classroom(self, classroom_id, student_username):
        '''Ajouter √©l√®ve'''
    
    def create_assignment(self, classroom_id, title, skill_domains,
                         difficulty, exercise_count, due_date):
        '''Cr√©er et assigner exercices √† toute la classe'''
    
    def get_classroom_overview(self, classroom_id):
        '''Real-time stats: student progress, success rates, etc.'''
    
    def generate_competency_report(self, classroom_id):
        '''Export CSV/PDF: qui a ma√Ætris√© quelles comp√©tences?'''
```

## Database Tables (Alembic migrations)

Already defined in 7.1, but integrate with app:
- classrooms
- classroom_enrollments
- assignments
- assignment_responses
- curriculum_competencies
- student_competency_progress

## Real-time Monitoring

```python
def get_classroom_overview(classroom_id):
    students = db.query(classroom_enrollments, classroom_id)
    
    stats = []
    for student in students:
        recent = db.query(exercise_responses,
                         student_id, created_at > 7 days)
        success_rate = sum(1 for e in recent if e.is_correct) / len(recent)
        
        stats.append({
            "student": student.username,
            "recent_success_rate": success_rate,
            "exercises_week": len(recent),
            "current_focus": get_focus_domain(student_id)
        })
    
    return {
        "total_students": len(students),
        "class_avg": mean([s['success_rate'] for s in stats]),
        "student_stats": stats
    }
```

## Tests
`tests/test_classroom_manager.py` (400+ tests)

- Classroom CRUD
- Assignment creation & distribution
- Permission checks
- Real-time metrics accuracy
```

---

### Prompt 8.1.2 - Teacher Dashboard UI

**Titre:** "Cr√©er Teacher Dashboard Streamlit"

**Texte:**

```
# T√ÇCHE: Teacher Dashboard UI

Streamlit interface pour enseignants.

## Fichier √† cr√©er
`ui/teacher_dashboard.py` (400 lignes)

## Layout

### Sidebar
- Classroom selector dropdown
- Navigation menu

### Tabs

**Tab 1: Overview**
- Total students card
- Class avg success rate card
- Weekly activity card
- Student grid table:
  | Nom | Taux Succ√®s | Exercices Semaine | Domaine Actuel |
  |-----|------------|-------------------|-----------------|

**Tab 2: Assignments**
- Create assignment form:
  - Title input
  - Skill domains multiselect
  - Difficulty slider (1-5)
  - Exercise count number input
  - Due date picker
- List of existing assignments

**Tab 3: Students**
- Add student form
- Student list with actions (remove, view detail)
- Individual student progress chart

**Tab 4: Reports**
- Generate buttons:
  - "G√©n√©rer Rapport Comp√©tences"
  - "G√©n√©rer Rapport Classe"
  - "G√©n√©rer Attestations"
- Download buttons for generated files

## UI Code Structure

```python
def render_teacher_dashboard():
    st.set_page_config(page_title="Tableau Bord Enseignant", layout="wide")
    
    # Sidebar
    classroom = st.sidebar.selectbox("Classe", classrooms)
    classroom_id = get_classroom_id(classroom)
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs(["Overview", "Assignments", "Students", "Reports"])
    
    with tab1:
        overview = cm.get_classroom_overview(classroom_id)
        # Display metrics + student grid
    
    with tab2:
        # Create assignment form
    
    # ... etc
```

## Tests
`tests/test_teacher_dashboard.py` (200+ tests)

- UI rendering
- Form submission
- Data validation
- Permission checks
```

---

## 8.2 - ANALYTICS DASHBOARD

### Prompt 8.2.1 - Analytics Engine & Visualizations

**Titre:** "Cr√©er AnalyticsEngine + Plotly visualizations"

**Texte:**

```
# T√ÇCHE: Analytics Dashboard Engine

Advanced analytics pour insights p√©dagogiques.

## Fichier √† cr√©er
`core/analytics/analytics_engine.py` (500 lignes)

## Visualizations Types

1. **Progress Trajectories** (Line chart)
   - Student proficiency over time
   - By skill domain

2. **Heatmaps** (Skill mastery)
   - Students (rows) √ó Domains (columns)
   - Color: Red (0%) ‚Üí Green (100%)

3. **Distributions**
   - Success rate distribution
   - Time taken distribution

4. **Comparative**
   - Student vs class average
   - Domain vs domain

5. **Predictive**
   - Mastery timeline forecast
   - At-risk student forecast

## Analytics Engine

```python
class AnalyticsEngine:
    def generate_progress_trajectory(self, user_id, domain):
        '''Retourne DataFrame avec progres temporelle'''
    
    def generate_skill_heatmap(self, classroom_id):
        '''Retourne heatmap array (students √ó domains)'''
    
    def generate_comparative_report(self, user_id, classroom_id):
        '''Compare user vs class benchmark'''
    
    def generate_predictive_forecast(self, user_id):
        '''Forecast when mastery achieved'''
```

## Plotly Visualizations

```python
import plotly.graph_objects as go
import plotly.express as px

# Line chart
fig = px.line(df, x='date', y='proficiency',
              title="Progress Trajectory",
              labels={'proficiency': 'Proficiency Level'})

# Heatmap
fig = go.Figure(data=go.Heatmap(
    z=heatmap_data,
    x=domains,
    y=students,
    colorscale='RdYlGn',
    colorbar=dict(title="Ma√Ætrise")
))

# Export to HTML/image
```

## Tests
`tests/test_analytics.py` (300+ tests)

- Trajectory calculation accuracy
- Heatmap generation
- Comparative metric accuracy
- Forecast validation
```

---

### Prompt 8.2.2 - Dashboard UI & Integration

**Titre:** "Cr√©er Analytics Dashboard Streamlit"

**Texte:**

```
# T√ÇCHE: Analytics Dashboard UI

Streamlit interface pour visualizer analytics.

## Fichier √† cr√©er
`ui/analytics_dashboard.py` (400 lignes)

## Layout

### Filters (Top)
- Time range: [1 week | 1 month | 3 months | All]
- View type: [Student | Class | Domain]
- Domain multiselect

### Tabs

**Tab 1: Trajectories**
- Line charts for each selected domain
- Student proficiency over time

**Tab 2: Heatmaps**
- Skill mastery heatmap
- Students vs Domains

**Tab 3: Comparative**
- Student vs class metrics
- Domain comparison bar charts

**Tab 4: Predictive**
- Mastery timeline forecast
- At-risk student detection

## Streamlit Code

```python
def render_analytics_dashboard():
    st.set_page_config(page_title="Analytics", layout="wide")
    
    # Filters
    time_range = st.selectbox("Period", ["1 week", "1 month", "3 months", "All"])
    view_type = st.selectbox("View", ["Student", "Class", "Domain"])
    domains = st.multiselect("Domains", [...])
    
    # Tabs
    tab1, tab2, tab3, tab4 = st.tabs(["Trajectories", "Heatmaps", "Comparative", "Predictive"])
    
    with tab1:
        for domain in domains:
            df = ae.generate_progress_trajectory(user_id, domain)
            fig = px.line(df, x='date', y='proficiency')
            st.plotly_chart(fig)
    
    # ... etc tabs
```

## Tests
`tests/test_analytics_dashboard.py` (200+ tests)

- Dashboard rendering
- Filter functionality
- Chart generation
- Export functionality
```

---

# üìù UTILISATION DES PROMPTS

## Pour Chaque T√¢che

1. **Copier le prompt** (section "Texte du Prompt")
2. **Aller sur Claude Code**
3. **Coller le prompt enti√®rement**
4. **Ajouter contexte si besoin**: "Je travaille sur MathCopain, une app Streamlit d'apprentissage maths..."
5. **Laisser Claude Code g√©n√©rer**
6. **V√©rifier:**
   - [ ] Code compiles
   - [ ] Tous les tests passent
   - [ ] Coverage 85%+
   - [ ] No console errors
7. **Commit sur git**

## Template de Feedback √† Claude Code

Si quelque chose ne marche pas:

```
‚ùå Erreur: [description]

Fix needed:
1. [Issue]
2. [Context]
3. [What I need]

Je dois avoir:
- [ ] Code qui compile
- [ ] Tests 100% passing
- [ ] Coverage 85%+
```

---

# üéØ ORDRE D'EX√âCUTION RECOMMAND√â

**Phase 6 Sequential:**
1. 6.1.1 - ErrorAnalyzer
2. 6.1.2 - error_taxonomy.json
3. 6.1.3 - FeedbackGenerator
4. 6.1.4 - App integration
5. 6.2.1 - MetacognitionEngine
6. 6.3.1 - LearningStyleAnalyzer
7. 6.3.2 - 5 Adapters

**Phase 7 Sequential:**
1. 7.1.1 - PostgreSQL models
2. 7.1.2 - Migration script
3. 7.2.1 - DifficultyOptimizer
4. 7.2.2 - PerformancePredictor

**Phase 8 Sequential:**
1. 8.1.1 - ClassroomManager
2. 8.1.2 - Teacher Dashboard
3. 8.2.1 - Analytics Engine
4. 8.2.2 - Analytics Dashboard

---

**G√©n√©r√©:** 2025-11-15  
**Prompts Totaux:** 15+  
**Coverage Attendu:** 85%+  
**Tests Attendus:** 3,350+

"""
ErrorAnalyzer - Analyse intelligente des erreurs math√©matiques
Phase 6.1 - MathCopain v6.4

Bas√© sur la recherche de Hattie 2008 (effet-taille feedback: 0.79)
Transforme erreurs ‚Üí insights p√©dagogiques actionnables
"""

import json
import os
import re
from typing import Dict, List, Optional, Any, Tuple
from pathlib import Path
import numpy as np
from dataclasses import dataclass, asdict


@dataclass
class ErrorAnalysisResult:
    """R√©sultat d'analyse d'une erreur math√©matique"""
    error_type: str  # "Conceptual" | "Procedural" | "Calculation"
    misconception: str
    severity: int  # 1-5
    confidence: float  # 0.0-1.0
    examples: List[str]
    prerequisites_gaps: List[str]
    remediation_strategy: str
    common_ages: List[str]
    error_id: Optional[str] = None
    root_cause: Optional[str] = None
    feedback_templates: Optional[List[str]] = None
    remediation_path: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convertit en dictionnaire pour s√©rialisation"""
        return asdict(self)


class ErrorAnalyzer:
    """
    Analyse les erreurs math√©matiques et cat√©gorise par type

    Capacit√©s:
    - D√©tection automatique du type d'erreur (Conceptual/Procedural/Calculation)
    - Identification des misconceptions sous-jacentes
    - Analyse des causes racines
    - Recommandations de rem√©diation personnalis√©es

    Bas√© sur catalogue de 500+ erreurs pr√©-catalogu√©es
    """

    def __init__(self, taxonomy_path: Optional[str] = None):
        """
        Initialise l'analyseur d'erreurs

        Args:
            taxonomy_path: Chemin vers error_taxonomy.json (optionnel)
        """
        if taxonomy_path is None:
            # Chemin par d√©faut relatif √† ce fichier
            current_dir = Path(__file__).parent.parent.parent
            taxonomy_path = current_dir / "data" / "error_taxonomy.json"

        self.taxonomy_path = Path(taxonomy_path)
        self.error_catalog = self._load_catalog()
        self.detection_patterns = self.error_catalog.get("detection_patterns", {})
        self.remediation_strategies = self.error_catalog.get("remediation_strategies", {})

    def _load_catalog(self) -> Dict[str, Any]:
        """
        Charge le catalogue d'erreurs depuis error_taxonomy.json

        Returns:
            Dictionnaire contenant toute la taxonomie

        Raises:
            FileNotFoundError: Si le fichier n'existe pas
            json.JSONDecodeError: Si le JSON est invalide
        """
        if not self.taxonomy_path.exists():
            raise FileNotFoundError(
                f"Catalogue d'erreurs non trouv√©: {self.taxonomy_path}"
            )

        with open(self.taxonomy_path, 'r', encoding='utf-8') as f:
            catalog = json.load(f)

        # Validation basique
        if "error_categories" not in catalog:
            raise ValueError("Format de catalogue invalide: 'error_categories' manquant")

        return catalog

    def analyze_error_type(
        self,
        exercise: Dict[str, Any],
        response: Any,
        expected: Any
    ) -> ErrorAnalysisResult:
        """
        Analyse une erreur et d√©termine son type

        Args:
            exercise: Dictionnaire d√©crivant l'exercice
                {
                    "type": "addition" | "subtraction" | "multiplication" | etc.,
                    "operation": "27 + 48",
                    "difficulty": "CE2",
                    "context": "addition avec retenue"
                }
            response: R√©ponse de l'√©l√®ve
            expected: R√©ponse attendue

        Returns:
            ErrorAnalysisResult avec tous les d√©tails de l'analyse
        """
        # Extraire type d'exercice
        exercise_type = exercise.get("type", "unknown")
        operation = exercise.get("operation", "")
        difficulty = exercise.get("difficulty", "CE2")

        # Convertir r√©ponses en nombres si possible
        response_val = self._parse_number(response)
        expected_val = self._parse_number(expected)

        # Analyser l'√©cart
        difference = None
        if response_val is not None and expected_val is not None:
            difference = abs(response_val - expected_val)

        # Chercher patterns dans toutes les cat√©gories
        candidates = []

        for category_name, category_data in self.error_catalog["error_categories"].items():
            if "errors" not in category_data:
                continue

            # Chercher dans le bon domaine math√©matique
            domain = self._map_exercise_type_to_domain(exercise_type)
            if domain in category_data["errors"]:
                errors_list = category_data["errors"][domain]

                for error_def in errors_list:
                    # Calculer score de match
                    match_score = self._calculate_match_score(
                        error_def,
                        exercise,
                        response,
                        expected,
                        difference
                    )

                    if match_score > 0.3:  # Seuil minimum
                        candidates.append({
                            "category": category_name,
                            "error": error_def,
                            "score": match_score
                        })

        # S√©lectionner meilleur candidat
        if candidates:
            best_match = max(candidates, key=lambda x: x["score"])
            return self._build_result_from_match(best_match, difficulty)

        # Fallback: erreur de calcul g√©n√©rique
        return self._create_generic_calculation_error(response, expected, difficulty)

    def identify_misconception(self, error_type: str, error_id: Optional[str] = None) -> Dict[str, Any]:
        """
        Identifie la misconception associ√©e √† un type d'erreur

        Args:
            error_type: Type d'erreur ("Conceptual", "Procedural", "Calculation")
            error_id: ID sp√©cifique d'erreur (optionnel, ex: "ADD_CONC_001")

        Returns:
            Dictionnaire avec d√©tails de la misconception:
            {
                "misconception": "description",
                "common_reasons": [...],
                "examples": [...],
                "severity": 1-5
            }
        """
        if error_id:
            # Recherche par ID sp√©cifique
            error_details = self._find_error_by_id(error_id)
            if error_details:
                return {
                    "misconception": error_details.get("misconception", ""),
                    "common_reasons": [error_details.get("pattern", "")],
                    "examples": error_details.get("examples", []),
                    "severity": error_details.get("severity", 3),
                    "prerequisites": error_details.get("prerequisites", [])
                }

        # Recherche par cat√©gorie
        if error_type in self.error_catalog.get("error_categories", {}):
            category = self.error_catalog["error_categories"][error_type]
            return {
                "misconception": category.get("description", ""),
                "common_reasons": [],
                "examples": [],
                "severity": 3,
                "prerequisites": []
            }

        return {
            "misconception": "Erreur non identifi√©e",
            "common_reasons": [],
            "examples": [],
            "severity": 2,
            "prerequisites": []
        }

    def root_cause_analysis(self, error_details: ErrorAnalysisResult) -> Dict[str, Any]:
        """
        Analyse la cause racine d'une erreur

        Args:
            error_details: R√©sultat d'analyse d'erreur

        Returns:
            Dictionnaire avec analyse approfondie:
            {
                "root_cause": "description de la cause principale",
                "contributing_factors": [...],
                "prerequisite_gaps": [...],
                "recommended_actions": [...],
                "estimated_remediation_time": "2-4 sessions"
            }
        """
        error_type = error_details.error_type
        severity = error_details.severity
        prerequisites = error_details.prerequisites_gaps

        # D√©terminer cause racine selon type
        if error_type == "Conceptual":
            root_cause = "L'enfant n'a pas construit une compr√©hension solide du concept math√©matique sous-jacent"
            contributing_factors = [
                "Manque de manipulations concr√®tes",
                "Passage trop rapide √† l'abstraction",
                "Pr√©requis non ma√Ætris√©s"
            ]
        elif error_type == "Procedural":
            root_cause = "L'enfant conna√Æt le concept mais ex√©cute incorrectement l'algorithme"
            contributing_factors = [
                "Manque de pratique guid√©e",
                "Oubli d'√©tapes interm√©diaires",
                "Automatisation insuffisante"
            ]
        else:  # Calculation
            root_cause = "Erreur d'inattention ou lacune en calcul mental"
            contributing_factors = [
                "Fatigue ou manque de concentration",
                "Tables non automatis√©es",
                "V√©rification insuffisante"
            ]

        # Estimer temps de rem√©diation
        remediation_time = self._estimate_remediation_time(severity, error_type)

        # Actions recommand√©es
        recommended_actions = self._get_recommended_actions(
            error_type,
            severity,
            error_details.remediation_strategy
        )

        return {
            "root_cause": root_cause,
            "contributing_factors": contributing_factors,
            "prerequisite_gaps": prerequisites,
            "recommended_actions": recommended_actions,
            "estimated_remediation_time": remediation_time,
            "severity_level": self._severity_label(severity),
            "urgency": "haute" if severity >= 4 else "moyenne" if severity >= 3 else "faible"
        }

    def _map_exercise_type_to_domain(self, exercise_type: str) -> str:
        """Mappe type d'exercice vers domaine dans taxonomie"""
        mapping = {
            "addition": "addition",
            "subtraction": "subtraction",
            "multiplication": "multiplication",
            "division": "division",
            "fractions": "fractions",
            "decimals": "decimals",
            "geometry": "geometry",
            "measurement": "measurement",
            "mesures": "measurement",
            "geometrie": "geometry"
        }
        return mapping.get(exercise_type.lower(), "arithmetic")

    def _parse_number(self, value: Any) -> Optional[float]:
        """Parse une valeur en nombre"""
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, str):
            try:
                # G√©rer fractions
                if '/' in value:
                    parts = value.split('/')
                    return float(parts[0]) / float(parts[1])
                # Remplacer virgule par point
                value = value.replace(',', '.')
                return float(value)
            except (ValueError, ZeroDivisionError):
                return None
        return None

    def _calculate_match_score(
        self,
        error_def: Dict[str, Any],
        exercise: Dict[str, Any],
        response: Any,
        expected: Any,
        difference: Optional[float]
    ) -> float:
        """
        Calcule score de correspondance entre erreur observ√©e et d√©finition

        Returns:
            Score entre 0.0 et 1.0
        """
        score = 0.0

        # V√©rifier √¢ge appropri√©
        difficulty = exercise.get("difficulty", "CE2")
        if difficulty in error_def.get("common_ages", []):
            score += 0.3

        # V√©rifier pattern dans operation
        pattern = error_def.get("pattern", "")
        operation = exercise.get("operation", "")
        context = exercise.get("context", "")

        if pattern.lower() in operation.lower() or pattern.lower() in context.lower():
            score += 0.4

        # Score bas√© sur diff√©rence si erreur de calcul
        if difference is not None and difference > 0:
            # Petite diff√©rence = probablement erreur calcul
            # Grande diff√©rence = probablement erreur conceptuelle
            if error_def.get("id", "").startswith("CALC") and difference <= 5:
                score += 0.3
            elif not error_def.get("id", "").startswith("CALC") and difference > 5:
                score += 0.2

        # Utiliser confidence de d√©tection
        detection_conf = error_def.get("detection_confidence", 0.5)
        score = score * detection_conf

        return min(score, 1.0)

    def _build_result_from_match(self, match: Dict[str, Any], difficulty: str) -> ErrorAnalysisResult:
        """Construit ErrorAnalysisResult depuis un match"""
        error_def = match["error"]
        category = match["category"]

        return ErrorAnalysisResult(
            error_type=category,
            misconception=error_def.get("misconception", ""),
            severity=error_def.get("severity", 3),
            confidence=match["score"],
            examples=error_def.get("examples", []),
            prerequisites_gaps=error_def.get("prerequisites", []),
            remediation_strategy=error_def.get("remediation_strategy", "pratique_guidee"),
            common_ages=error_def.get("common_ages", [difficulty]),
            error_id=error_def.get("id"),
            root_cause=None,  # Sera rempli par root_cause_analysis
            feedback_templates=error_def.get("feedback_templates", []),
            remediation_path=error_def.get("remediation_path", "")
        )

    def _create_generic_calculation_error(
        self,
        response: Any,
        expected: Any,
        difficulty: str
    ) -> ErrorAnalysisResult:
        """Cr√©e un r√©sultat g√©n√©rique pour erreur de calcul"""
        generic_templates = [
            f"Tu as r√©pondu {response}, mais la bonne r√©ponse est {expected}. V√©rifie ton calcul!",
            "Petite erreur de calcul. Reprends √©tape par √©tape.",
            "Prends ton temps pour bien calculer. Tu y es presque!",
            "Pas de souci! Cette erreur arrive souvent. Continue √† pratiquer."
        ]

        return ErrorAnalysisResult(
            error_type="Calculation",
            misconception="Erreur de calcul ou d'inattention",
            severity=2,
            confidence=0.60,
            examples=[f"R√©pondu {response} au lieu de {expected}"],
            prerequisites_gaps=["calcul_mental", "attention"],
            remediation_strategy="pratique_reguliere",
            common_ages=[difficulty],
            error_id="CALC_001",
            root_cause="Erreur de calcul mental ou manque d'attention",
            feedback_templates=generic_templates,
            remediation_path="calculation_skills_practice"
        )

    def _find_error_by_id(self, error_id: str) -> Optional[Dict[str, Any]]:
        """Recherche une erreur par son ID"""
        for category_name, category_data in self.error_catalog.get("error_categories", {}).items():
            if "errors" not in category_data:
                continue

            for domain, errors_list in category_data["errors"].items():
                for error_def in errors_list:
                    if error_def.get("id") == error_id:
                        return error_def

        return None

    def _estimate_remediation_time(self, severity: int, error_type: str) -> str:
        """Estime le temps de rem√©diation n√©cessaire"""
        base_sessions = {
            "Conceptual": 4,
            "Procedural": 3,
            "Calculation": 2
        }

        sessions = base_sessions.get(error_type, 3) + (severity - 3)
        sessions = max(1, sessions)

        if sessions <= 2:
            return "1-2 sessions"
        elif sessions <= 4:
            return "3-4 sessions"
        elif sessions <= 6:
            return "5-6 sessions"
        else:
            return "7+ sessions"

    def _get_recommended_actions(
        self,
        error_type: str,
        severity: int,
        strategy_key: str
    ) -> List[str]:
        """G√©n√®re liste d'actions recommand√©es"""
        actions = []

        # Actions bas√©es sur strat√©gie
        strategy = self.remediation_strategies.get(strategy_key, {})
        if strategy:
            actions.append(f"Utiliser: {strategy.get('description', strategy_key)}")

        # Actions bas√©es sur type
        if error_type == "Conceptual":
            actions.extend([
                "Revenir aux manipulations concr√®tes",
                "Utiliser repr√©sentations visuelles vari√©es",
                "Faire verbaliser le raisonnement"
            ])
        elif error_type == "Procedural":
            actions.extend([
                "D√©composer l'algorithme en √©tapes explicites",
                "Pratiquer avec guidance progressive",
                "Automatiser par r√©p√©tition espac√©e"
            ])
        else:  # Calculation
            actions.extend([
                "R√©viser les tables et calcul mental",
                "Encourager la v√©rification syst√©matique",
                "Pauses r√©guli√®res pour maintenir concentration"
            ])

        # Actions bas√©es sur s√©v√©rit√©
        if severity >= 4:
            actions.insert(0, "‚ö†Ô∏è PRIORIT√â HAUTE - Traiter rapidement")

        return actions[:5]  # Limiter √† 5 actions principales

    def _severity_label(self, severity: int) -> str:
        """Convertit niveau de s√©v√©rit√© en label"""
        labels = {
            1: "Tr√®s faible",
            2: "Faible",
            3: "Mod√©r√©e",
            4: "√âlev√©e",
            5: "Tr√®s √©lev√©e"
        }
        return labels.get(severity, "Mod√©r√©e")

    def generate_personalized_feedback(
        self,
        error_analysis: ErrorAnalysisResult,
        student_name: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        G√©n√®re un feedback personnalis√© bas√© sur l'analyse d'erreur

        Args:
            error_analysis: R√©sultat de l'analyse d'erreur
            student_name: Nom de l'√©l√®ve (optionnel)
            context: Contexte additionnel (exercice, r√©ponse, etc.)

        Returns:
            Message de feedback personnalis√© et p√©dagogique
        """
        # R√©cup√©rer templates
        templates = error_analysis.feedback_templates or []

        if not templates:
            # Fallback si pas de templates
            return self._generate_generic_feedback(error_analysis, student_name)

        # Choisir un template (premier par d√©faut, ou al√©atoire pour vari√©t√©)
        import random
        template = random.choice(templates) if len(templates) > 1 else templates[0]

        # Personnaliser avec nom si fourni
        if student_name:
            greeting = f"{student_name}, "
        else:
            greeting = ""

        # Remplacer variables dans template si contexte fourni
        feedback = template
        if context:
            try:
                # Tentative de formatage avec contexte
                feedback = template.format(**context)
            except (KeyError, ValueError):
                # Si formatage √©choue, garder template original
                pass

        # Ajouter encouragement selon s√©v√©rit√©
        encouragement = self._get_encouragement(error_analysis.severity)

        return f"{greeting}{feedback}\n\n{encouragement}"

    def _generate_generic_feedback(
        self,
        error_analysis: ErrorAnalysisResult,
        student_name: Optional[str] = None
    ) -> str:
        """G√©n√®re feedback g√©n√©rique si pas de templates"""
        name = f"{student_name}, " if student_name else ""

        feedback_parts = [
            f"{name}tu as fait une erreur de type {error_analysis.error_type.lower()}.",
            f"Misconception identifi√©e: {error_analysis.misconception}",
            f"Pour progresser: {error_analysis.remediation_strategy}"
        ]

        return "\n".join(feedback_parts)

    def _get_encouragement(self, severity: int) -> str:
        """Retourne un message d'encouragement selon la s√©v√©rit√©"""
        if severity >= 4:
            return "üí™ Cette notion est importante. Prenons le temps de bien la comprendre ensemble!"
        elif severity >= 3:
            return "üëç Pas de souci! Avec un peu de pratique, tu vas y arriver."
        else:
            return "‚ú® Bravo pour ton effort! Continue comme √ßa."

    def get_statistics(self) -> Dict[str, Any]:
        """Retourne statistiques sur le catalogue d'erreurs"""
        total_errors = 0
        by_category = {}
        by_domain = {}

        for category_name, category_data in self.error_catalog.get("error_categories", {}).items():
            category_count = 0
            if "errors" in category_data:
                for domain, errors_list in category_data["errors"].items():
                    count = len(errors_list)
                    category_count += count
                    by_domain[domain] = by_domain.get(domain, 0) + count

            by_category[category_name] = category_count
            total_errors += category_count

        return {
            "total_errors": total_errors,
            "by_category": by_category,
            "by_domain": by_domain,
            "version": self.error_catalog.get("version", "unknown")
        }
